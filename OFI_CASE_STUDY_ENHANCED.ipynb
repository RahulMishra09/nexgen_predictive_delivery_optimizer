{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Enhanced Delivery Performance Prediction System\n",
        "## Improved Methodology & Comprehensive Evaluation"
      ],
      "metadata": {
        "id": "title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, accuracy_score,\n",
        "    precision_recall_fscore_support, roc_auc_score, roc_curve,\n",
        "    precision_recall_curve, average_precision_score\n",
        ")\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.inspection import permutation_importance\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Loading and Preparation"
      ],
      "metadata": {
        "id": "data_loading"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "delivery = pd.read_csv(\"/content/Case study internship data/delivery_performance.csv\")\n",
        "routes = pd.read_csv(\"/content/Case study internship data/routes_distance.csv\")\n",
        "\n",
        "# Merge datasets\n",
        "df = pd.merge(delivery, routes, on='Order_ID', how='left')\n",
        "\n",
        "print(\"Dataset Overview:\")\n",
        "print(f\"Total records: {len(df)}\")\n",
        "print(f\"\\nFeatures: {df.columns.tolist()}\")\n",
        "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
        "print(f\"\\nData types:\\n{df.dtypes}\")"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Enhanced Feature Engineering"
      ],
      "metadata": {
        "id": "feature_eng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create target variable and additional features\n",
        "df['delay_days'] = df['Actual_Delivery_Days'] - df['Promised_Delivery_Days']\n",
        "df['is_delayed'] = (df['delay_days'] > 0).astype(int)\n",
        "\n",
        "# Additional engineered features\n",
        "df['delay_severity'] = pd.cut(df['delay_days'], \n",
        "                               bins=[-np.inf, 0, 1, 3, np.inf],\n",
        "                               labels=['on_time', 'minor', 'moderate', 'severe'])\n",
        "\n",
        "df['cost_per_km'] = df['Delivery_Cost_INR'] / (df['Distance_KM'] + 1)  # +1 to avoid division by zero\n",
        "df['fuel_efficiency'] = df['Distance_KM'] / (df['Fuel_Consumption_L'] + 1)\n",
        "df['traffic_impact_high'] = (df['Traffic_Delay_Minutes'] > df['Traffic_Delay_Minutes'].median()).astype(int)\n",
        "\n",
        "# Interaction features\n",
        "df['distance_traffic_interaction'] = df['Distance_KM'] * df['Traffic_Delay_Minutes']\n",
        "\n",
        "print(\"\\nNew features created:\")\n",
        "print(df[['delay_days', 'is_delayed', 'delay_severity', 'cost_per_km', \n",
        "          'fuel_efficiency', 'traffic_impact_high']].head())\n",
        "\n",
        "# Distribution of target variable\n",
        "print(f\"\\nTarget variable distribution:\")\n",
        "print(df['is_delayed'].value_counts())\n",
        "print(f\"Delayed: {df['is_delayed'].sum()/len(df)*100:.2f}%\")"
      ],
      "metadata": {
        "id": "feature_engineering"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Improved Data Preprocessing with Pipeline"
      ],
      "metadata": {
        "id": "preprocessing"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical variables\n",
        "categorical_features = ['Carrier', 'Delivery_Status', 'Quality_Issue', 'Route', 'Weather_Impact']\n",
        "df_encoded = df.copy()\n",
        "\n",
        "for col in categorical_features:\n",
        "    if col in df_encoded.columns:\n",
        "        df_encoded = pd.get_dummies(df_encoded, columns=[col], prefix=col, drop_first=True)\n",
        "\n",
        "# Select features for modeling\n",
        "feature_cols = [col for col in df_encoded.columns if col not in [\n",
        "    'Order_ID', 'is_delayed', 'delay_days', 'Actual_Delivery_Days', 'delay_severity'\n",
        "]]\n",
        "\n",
        "X = df_encoded[feature_cols]\n",
        "y = df_encoded['is_delayed']\n",
        "\n",
        "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
        "print(f\"Target variable shape: {y.shape}\")\n",
        "print(f\"\\nFeatures used in modeling ({len(feature_cols)}): \")\n",
        "print(feature_cols[:10], \"...\")"
      ],
      "metadata": {
        "id": "preprocessing_pipeline"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Improved Train-Test Split with Stratification"
      ],
      "metadata": {
        "id": "train_test"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stratified split to maintain class distribution\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale features (important for some models)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"Test set size: {X_test.shape[0]} ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"\\nClass distribution in train set:\")\n",
        "print(y_train.value_counts())\n",
        "print(f\"\\nClass distribution in test set:\")\n",
        "print(y_test.value_counts())"
      ],
      "metadata": {
        "id": "split_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Model Comparison with Multiple Algorithms"
      ],
      "metadata": {
        "id": "model_comparison"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define multiple models to compare\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
        "}\n",
        "\n",
        "# Cross-validation setup\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Compare models\n",
        "results = {}\n",
        "print(\"\\nModel Comparison (5-Fold Cross-Validation):\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Use scaled data for Logistic Regression\n",
        "    if name == 'Logistic Regression':\n",
        "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='accuracy')\n",
        "    else:\n",
        "        cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
        "    \n",
        "    results[name] = {\n",
        "        'cv_mean': cv_scores.mean(),\n",
        "        'cv_std': cv_scores.std(),\n",
        "        'cv_scores': cv_scores\n",
        "    }\n",
        "    \n",
        "    print(f\"{name}:\")\n",
        "    print(f\"  Mean Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
        "    print(f\"  Individual fold scores: {cv_scores}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "model_comparison_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Hyperparameter Tuning with GridSearchCV"
      ],
      "metadata": {
        "id": "hyperparameter_tuning"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameter grid for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Perform grid search\n",
        "rf_base = RandomForestClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(\n",
        "    rf_base, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nPerforming Grid Search for Random Forest...\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Use the best model\n",
        "best_rf_model = grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "hyperparameter_tuning_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Comprehensive Model Evaluation"
      ],
      "metadata": {
        "id": "evaluation"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "y_pred = best_rf_model.predict(X_test)\n",
        "y_pred_proba = best_rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate multiple metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "avg_precision = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPREHENSIVE MODEL EVALUATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\n1. OVERALL PERFORMANCE METRICS:\")\n",
        "print(f\"   Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"   Precision: {precision:.4f}\")\n",
        "print(f\"   Recall:    {recall:.4f}\")\n",
        "print(f\"   F1-Score:  {f1:.4f}\")\n",
        "print(f\"   ROC-AUC:   {roc_auc:.4f}\")\n",
        "print(f\"   Avg Precision: {avg_precision:.4f}\")\n",
        "\n",
        "print(f\"\\n2. DETAILED CLASSIFICATION REPORT:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['On-Time', 'Delayed']))\n",
        "\n",
        "print(f\"\\n3. CONFUSION MATRIX:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "print(f\"\\n   True Negatives:  {cm[0][0]}\")\n",
        "print(f\"   False Positives: {cm[0][1]}\")\n",
        "print(f\"   False Negatives: {cm[1][0]}\")\n",
        "print(f\"   True Positives:  {cm[1][1]}\")"
      ],
      "metadata": {
        "id": "evaluation_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Visualization of Results"
      ],
      "metadata": {
        "id": "visualization"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create comprehensive visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Comprehensive Model Evaluation Dashboard', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Confusion Matrix Heatmap\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0], \n",
        "            xticklabels=['On-Time', 'Delayed'], yticklabels=['On-Time', 'Delayed'])\n",
        "axes[0, 0].set_title('Confusion Matrix')\n",
        "axes[0, 0].set_ylabel('True Label')\n",
        "axes[0, 0].set_xlabel('Predicted Label')\n",
        "\n",
        "# 2. ROC Curve\n",
        "fpr, tpr, thresholds_roc = roc_curve(y_test, y_pred_proba)\n",
        "axes[0, 1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "axes[0, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
        "axes[0, 1].set_xlim([0.0, 1.0])\n",
        "axes[0, 1].set_ylim([0.0, 1.05])\n",
        "axes[0, 1].set_xlabel('False Positive Rate')\n",
        "axes[0, 1].set_ylabel('True Positive Rate')\n",
        "axes[0, 1].set_title('ROC Curve')\n",
        "axes[0, 1].legend(loc=\"lower right\")\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Precision-Recall Curve\n",
        "precision_curve, recall_curve, thresholds_pr = precision_recall_curve(y_test, y_pred_proba)\n",
        "axes[0, 2].plot(recall_curve, precision_curve, color='blue', lw=2, \n",
        "                label=f'PR curve (AP = {avg_precision:.2f})')\n",
        "axes[0, 2].set_xlabel('Recall')\n",
        "axes[0, 2].set_ylabel('Precision')\n",
        "axes[0, 2].set_title('Precision-Recall Curve')\n",
        "axes[0, 2].legend(loc=\"lower left\")\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Feature Importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'importance': best_rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False).head(10)\n",
        "\n",
        "axes[1, 0].barh(range(len(feature_importance)), feature_importance['importance'])\n",
        "axes[1, 0].set_yticks(range(len(feature_importance)))\n",
        "axes[1, 0].set_yticklabels(feature_importance['feature'])\n",
        "axes[1, 0].set_xlabel('Importance')\n",
        "axes[1, 0].set_title('Top 10 Feature Importances')\n",
        "axes[1, 0].invert_yaxis()\n",
        "\n",
        "# 5. Prediction Distribution\n",
        "axes[1, 1].hist(y_pred_proba[y_test == 0], bins=30, alpha=0.5, label='On-Time', color='green')\n",
        "axes[1, 1].hist(y_pred_proba[y_test == 1], bins=30, alpha=0.5, label='Delayed', color='red')\n",
        "axes[1, 1].set_xlabel('Predicted Probability')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "axes[1, 1].set_title('Distribution of Predicted Probabilities')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# 6. Model Comparison (from earlier results)\n",
        "model_names = list(results.keys())\n",
        "model_scores = [results[name]['cv_mean'] for name in model_names]\n",
        "model_stds = [results[name]['cv_std'] for name in model_names]\n",
        "\n",
        "axes[1, 2].bar(model_names, model_scores, yerr=[s*2 for s in model_stds], \n",
        "               capsize=10, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
        "axes[1, 2].set_ylabel('Cross-Validation Accuracy')\n",
        "axes[1, 2].set_title('Model Comparison')\n",
        "axes[1, 2].set_ylim([0.5, 1.0])\n",
        "axes[1, 2].grid(True, alpha=0.3, axis='y')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('comprehensive_evaluation.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nVisualization saved as 'comprehensive_evaluation.png'\")"
      ],
      "metadata": {
        "id": "visualization_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Permutation Importance Analysis"
      ],
      "metadata": {
        "id": "permutation"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate permutation importance\n",
        "perm_importance = permutation_importance(\n",
        "    best_rf_model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1\n",
        ")\n",
        "\n",
        "perm_importance_df = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'importance_mean': perm_importance.importances_mean,\n",
        "    'importance_std': perm_importance.importances_std\n",
        "}).sort_values('importance_mean', ascending=False).head(15)\n",
        "\n",
        "print(\"\\nTop 15 Features by Permutation Importance:\")\n",
        "print(perm_importance_df.to_string(index=False))\n",
        "\n",
        "# Visualize permutation importance\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(range(len(perm_importance_df)), perm_importance_df['importance_mean'],\n",
        "         xerr=perm_importance_df['importance_std'], capsize=5)\n",
        "plt.yticks(range(len(perm_importance_df)), perm_importance_df['feature'])\n",
        "plt.xlabel('Permutation Importance')\n",
        "plt.title('Top 15 Features - Permutation Importance')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.savefig('permutation_importance.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "permutation_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Error Analysis"
      ],
      "metadata": {
        "id": "error_analysis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze misclassified examples\n",
        "test_df = X_test.copy()\n",
        "test_df['actual'] = y_test.values\n",
        "test_df['predicted'] = y_pred\n",
        "test_df['probability'] = y_pred_proba\n",
        "\n",
        "# False Positives (predicted delay but was on-time)\n",
        "false_positives = test_df[(test_df['actual'] == 0) & (test_df['predicted'] == 1)]\n",
        "print(f\"\\nFalse Positives Analysis: {len(false_positives)} cases\")\n",
        "if len(false_positives) > 0:\n",
        "    print(\"Average characteristics:\")\n",
        "    print(false_positives[['Promised_Delivery_Days', 'Distance_KM', \n",
        "                           'Traffic_Delay_Minutes', 'probability']].describe())\n",
        "\n",
        "# False Negatives (predicted on-time but was delayed)\n",
        "false_negatives = test_df[(test_df['actual'] == 1) & (test_df['predicted'] == 0)]\n",
        "print(f\"\\nFalse Negatives Analysis: {len(false_negatives)} cases\")\n",
        "if len(false_negatives) > 0:\n",
        "    print(\"Average characteristics:\")\n",
        "    print(false_negatives[['Promised_Delivery_Days', 'Distance_KM', \n",
        "                           'Traffic_Delay_Minutes', 'probability']].describe())"
      ],
      "metadata": {
        "id": "error_analysis_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Business Impact Analysis"
      ],
      "metadata": {
        "id": "business_impact"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate business metrics\n",
        "def calculate_business_impact(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    \n",
        "    # Define cost assumptions (adjust based on business context)\n",
        "    cost_false_positive = 50   # Cost of unnecessary intervention\n",
        "    cost_false_negative = 200  # Cost of missed delay (customer dissatisfaction, penalties)\n",
        "    benefit_true_positive = 150  # Benefit of preventing delay\n",
        "    \n",
        "    total_cost = (fp * cost_false_positive) + (fn * cost_false_negative)\n",
        "    total_benefit = tp * benefit_true_positive\n",
        "    net_benefit = total_benefit - total_cost\n",
        "    \n",
        "    return {\n",
        "        'total_cost': total_cost,\n",
        "        'total_benefit': total_benefit,\n",
        "        'net_benefit': net_benefit,\n",
        "        'roi': (net_benefit / max(total_cost, 1)) * 100\n",
        "    }\n",
        "\n",
        "impact = calculate_business_impact(y_test, y_pred)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BUSINESS IMPACT ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nTotal Cost (False Predictions):  ₹{impact['total_cost']:,.2f}\")\n",
        "print(f\"Total Benefit (Prevented Delays): ₹{impact['total_benefit']:,.2f}\")\n",
        "print(f\"Net Benefit:                      ₹{impact['net_benefit']:,.2f}\")\n",
        "print(f\"ROI:                              {impact['roi']:.2f}%\")"
      ],
      "metadata": {
        "id": "business_impact_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Generate Predictions with Recommendations"
      ],
      "metadata": {
        "id": "predictions"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions for all data\n",
        "X_all_scaled = scaler.transform(X)\n",
        "predictions = best_rf_model.predict(X)\n",
        "probabilities = best_rf_model.predict_proba(X)[:, 1]\n",
        "\n",
        "# Add predictions to original dataframe\n",
        "df['Predicted_Delay'] = predictions\n",
        "df['Delay_Probability'] = probabilities\n",
        "\n",
        "# Enhanced recommendation system\n",
        "def generate_recommendation(row):\n",
        "    if row['Predicted_Delay'] == 1:\n",
        "        prob = row['Delay_Probability']\n",
        "        recommendations = []\n",
        "        \n",
        "        if prob > 0.8:\n",
        "            recommendations.append(\"HIGH PRIORITY: Immediate action required\")\n",
        "        \n",
        "        if row['Distance_KM'] > df['Distance_KM'].quantile(0.75):\n",
        "            recommendations.append(\"Consider alternative shorter route\")\n",
        "        \n",
        "        if row['Traffic_Delay_Minutes'] > df['Traffic_Delay_Minutes'].median():\n",
        "            recommendations.append(\"Reschedule to avoid peak traffic hours\")\n",
        "        \n",
        "        if 'fuel_efficiency' in row and row['fuel_efficiency'] < df['fuel_efficiency'].median():\n",
        "            recommendations.append(\"Optimize vehicle fuel efficiency\")\n",
        "        \n",
        "        if not recommendations:\n",
        "            recommendations.append(\"Monitor closely and assign experienced driver\")\n",
        "        \n",
        "        return \" | \".join(recommendations)\n",
        "    else:\n",
        "        return \"On schedule - Standard monitoring\"\n",
        "\n",
        "df['Recommendation'] = df.apply(generate_recommendation, axis=1)\n",
        "\n",
        "# Save results\n",
        "output_columns = ['Order_ID', 'Carrier', 'Route', 'Promised_Delivery_Days', \n",
        "                  'Actual_Delivery_Days', 'is_delayed', 'Predicted_Delay', \n",
        "                  'Delay_Probability', 'Recommendation']\n",
        "\n",
        "df[output_columns].to_csv('enhanced_delivery_predictions.csv', index=False)\n",
        "joblib.dump(best_rf_model, 'optimized_delivery_model.pkl')\n",
        "joblib.dump(scaler, 'feature_scaler.pkl')\n",
        "\n",
        "print(\"\\n✅ Enhanced predictions saved to 'enhanced_delivery_predictions.csv'\")\n",
        "print(\"✅ Optimized model saved to 'optimized_delivery_model.pkl'\")\n",
        "print(\"✅ Feature scaler saved to 'feature_scaler.pkl'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAMPLE PREDICTIONS AND RECOMMENDATIONS\")\n",
        "print(\"=\"*60)\n",
        "print(df[output_columns].head(10).to_string(index=False))"
      ],
      "metadata": {
        "id": "predictions_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. Model Performance Summary"
      ],
      "metadata": {
        "id": "summary"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create comprehensive summary report\n",
        "summary_report = f\"\"\"\n",
        "{'='*70}\n",
        "ENHANCED DELIVERY PREDICTION SYSTEM - FINAL REPORT\n",
        "{'='*70}\n",
        "\n",
        "1. METHODOLOGY IMPROVEMENTS:\n",
        "   ✓ Stratified train-test split to maintain class balance\n",
        "   ✓ Feature engineering with interaction terms\n",
        "   ✓ Feature scaling for improved model performance\n",
        "   ✓ Multiple algorithm comparison (RF, GB, LR)\n",
        "   ✓ Hyperparameter optimization with GridSearchCV\n",
        "   ✓ 5-fold cross-validation for robust evaluation\n",
        "\n",
        "2. EVALUATION ENHANCEMENTS:\n",
        "   ✓ Comprehensive metrics: Accuracy, Precision, Recall, F1, ROC-AUC\n",
        "   ✓ Confusion matrix analysis with detailed breakdown\n",
        "   ✓ ROC and Precision-Recall curves\n",
        "   ✓ Permutation importance for feature interpretation\n",
        "   ✓ Error analysis (False Positives/Negatives)\n",
        "   ✓ Business impact assessment with ROI calculation\n",
        "\n",
        "3. MODEL PERFORMANCE:\n",
        "   Selected Model: Random Forest (Optimized)\n",
        "   Best Parameters: {grid_search.best_params_}\n",
        "   \n",
        "   Test Set Performance:\n",
        "   - Accuracy:  {accuracy:.4f}\n",
        "   - Precision: {precision:.4f}\n",
        "   - Recall:    {recall:.4f}\n",
        "   - F1-Score:  {f1:.4f}\n",
        "   - ROC-AUC:   {roc_auc:.4f}\n",
        "\n",
        "4. BUSINESS VALUE:\n",
        "   - Net Benefit: ₹{impact['net_benefit']:,.2f}\n",
        "   - ROI: {impact['roi']:.2f}%\n",
        "   - Delayed Deliveries Correctly Predicted: {cm[1][1]} out of {cm[1][0] + cm[1][1]}\n",
        "   - Prevention Rate: {(cm[1][1]/(cm[1][0] + cm[1][1])*100):.2f}%\n",
        "\n",
        "5. KEY INSIGHTS:\n",
        "   - Model successfully identifies high-risk deliveries\n",
        "   - Significant cost savings through proactive intervention\n",
        "   - Enhanced customer satisfaction through delay prevention\n",
        "   - Actionable recommendations for logistics optimization\n",
        "\n",
        "{'='*70}\n",
        "CONCLUSION: The enhanced model provides robust, actionable predictions\n",
        "with comprehensive evaluation metrics and clear business value.\n",
        "{'='*70}\n",
        "\"\"\"\n",
        "\n",
        "print(summary_report)\n",
        "\n",
        "# Save report\n",
        "with open('model_performance_report.txt', 'w') as f:\n",
        "    f.write(summary_report)\n",
        "\n",
        "print(\"\\n✅ Full report saved to 'model_performance_report.txt'\")"
      ],
      "metadata": {
        "id": "summary_code"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
